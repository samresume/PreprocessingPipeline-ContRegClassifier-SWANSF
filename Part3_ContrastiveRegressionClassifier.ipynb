{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da34512",
   "metadata": {},
   "source": [
    "# ContReg Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a1516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "data = []\n",
    "labels = []\n",
    "flare_type_labels_list = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load processed data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_normalized_data.pkl\", 'rb') as f:\n",
    "        data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_binary_labels.pkl\", 'rb') as f:\n",
    "        labels.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_flare_type_labels.pkl\", 'rb') as f:\n",
    "        flare_type_labels_list.append(pickle.load(f))\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "test_flare_type_labels_list = []\n",
    "\n",
    "# Load processed data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_normalized_data.pkl\", 'rb') as f:\n",
    "        test_data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_binary_labels.pkl\", 'rb') as f:\n",
    "        test_labels.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_flare_type_labels.pkl\", 'rb') as f:\n",
    "        test_flare_type_labels_list.append(pickle.load(f))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3aa228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def TSS(TP,TN,FP,FN):\n",
    "    TSS_value = (TP / (TP + FN)) - (FP / (FP + TN))\n",
    "    return TSS_value\n",
    "\n",
    "def HSS1(TP,TN,FP,FN):\n",
    "    HSS1_value = (2 * (TP * TN - FP * FN)) / ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN))\n",
    "    return HSS1_value\n",
    "    \n",
    "def HSS2(TP,TN,FP,FN):\n",
    "    HSS2_value = (2 * (TP * TN - FP * FN)) / ((TP + FP) * (FN + TN) + (TP + FN) * (FP + TN))\n",
    "    return HSS2_value\n",
    "\n",
    "def GSS(TP,TN,FP,FN):\n",
    "    GSS_value = (TP - (TP + FP) * (TP + FN) / (TP + FP + FN + TN))\n",
    "    return GSS_value\n",
    "\n",
    "def Recall(TP,TN,FP,FN):\n",
    "    Recall_value = (TP) / (TP + FN)\n",
    "    return Recall_value\n",
    "\n",
    "def FPR(TP,TN,FP,FN):\n",
    "    fpr_value = (FP) / (FP + TN)\n",
    "    return fpr_value\n",
    "\n",
    "def Accuracy(TP,TN,FP,FN):\n",
    "    accuracy_value = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return accuracy_value\n",
    "\n",
    "def Precision(TP,TN,FP,FN):\n",
    "    precision_value = (TP) / (TP + FP)\n",
    "    return precision_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_training(name, X_train, Y_train, y_type_train, X_test, Y_test, y_type_test, training_func, num):\n",
    "    kfold = np.array([[1,2],[2,3],[3,4],[4,5]])\n",
    "\n",
    "    metrics = []\n",
    "    metrics_values = np.array([])\n",
    "    \n",
    "    for i in range(0, num):\n",
    "        train_index = kfold[i,0]\n",
    "        test_index = kfold[i,1]\n",
    "        metrics_values = training_func(X_train[train_index-1], Y_train[train_index-1], y_type_train[train_index-1], X_test[test_index-1], Y_test[test_index-1], y_type_test[test_index-1])\n",
    "        while (metrics_values[4] < 0.01):\n",
    "            metrics_values = training_func(X_train[train_index-1], Y_train[train_index-1], y_type_train[train_index-1], X_test[test_index-1], Y_test[test_index-1], y_type_test[test_index-1])\n",
    "        metrics.append(np.append(np.append(train_index, test_index), metrics_values))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef63614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(result, name):\n",
    "    data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/results/\"\n",
    "\n",
    "    with open(data_dir + name + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    for i in range(4):\n",
    "        print(\"TSS: \" + str(result[i][6]) + \"    Recall: \" + str(result[i][10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113dd8c",
   "metadata": {},
   "source": [
    "## ContReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f2cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, GlobalAveragePooling1D, LSTM, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "def contrastive_regression(X_train, y_train, y_type_train, X_test, y_test, y_type_test):\n",
    "    # Define triplet loss function\n",
    "    def triplet_loss(anchor, positives, negatives, margin=4.0):\n",
    "        # Reshape the inputs to combine the temporal and feature dimensions\n",
    "        anchor_flat = tf.reshape(anchor, [anchor.shape[0], -1])\n",
    "        positives_flat = tf.reshape(positives, [positives.shape[0], positives.shape[1], -1])\n",
    "        negatives_flat = tf.reshape(negatives, [negatives.shape[0], negatives.shape[1], -1])\n",
    "\n",
    "        # Normalize the vectors to unit length\n",
    "        anchor_normalized = tf.nn.l2_normalize(anchor_flat, axis=-1)\n",
    "        positives_normalized = tf.nn.l2_normalize(positives_flat, axis=-1)\n",
    "        negatives_normalized = tf.nn.l2_normalize(negatives_flat, axis=-1)\n",
    "\n",
    "        # Compute the cosine similarity\n",
    "        pos_similarity = tf.reduce_sum(anchor_normalized[:, tf.newaxis, :] * positives_normalized, axis=-1)\n",
    "        neg_similarity = tf.reduce_sum(anchor_normalized[:, tf.newaxis, :] * negatives_normalized, axis=-1)\n",
    "\n",
    "        # Convert cosine similarity to cosine distance\n",
    "        pos_distance = 1 - pos_similarity\n",
    "        neg_distance = 1 - neg_similarity\n",
    "\n",
    "        # Sum the distances to positives and negatives\n",
    "        pos_distance_sum = tf.reduce_sum(pos_distance, axis=-1)\n",
    "        neg_distance_sum = tf.reduce_sum(neg_distance, axis=-1)\n",
    "\n",
    "        # Compute the triplet loss with cosine distance\n",
    "        loss = tf.maximum(pos_distance_sum - neg_distance_sum + margin, 0.0)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    # Build GRU model with embedding and classification heads\n",
    "    def build_contrastive_model(input_shape, num_lstm_layers, lstm_units, dense_units, dropout_rate=0.3):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for _ in range(num_lstm_layers):\n",
    "            x = GRU(lstm_units, return_sequences=True)(x)\n",
    "            x = Dropout(dropout_rate)(x)  # Add dropout after each GRU layer\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        embeddings = Dense(dense_units, activation='relu')(x)\n",
    "\n",
    "        # Create model\n",
    "        model = Model(inputs, embeddings)\n",
    "        return model\n",
    "\n",
    "    # Build simple GRU model for normal learning\n",
    "    def build_regression_model(input_shape, num_gru_layers, gru_units, dropout_rate=0.3):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        for _ in range(num_gru_layers):\n",
    "            x = GRU(gru_units, return_sequences=True)(x)\n",
    "            x = Dropout(dropout_rate)(x)  # Add dropout after each GRU layer\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        mid = Dense(2, activation='relu')(x)\n",
    "        classification_output = Dense(1)(mid)\n",
    "\n",
    "        # Create model\n",
    "        model = Model(inputs, classification_output)\n",
    "        return model\n",
    "\n",
    "    # Combine contrastive and normal learning models\n",
    "    def combined_model(input_shape, num_layers, units, dense_units, dropout_rate=0.3):\n",
    "        contrastive_model = build_contrastive_model(input_shape, num_layers, units, dense_units, dropout_rate)\n",
    "        regression_model = build_regression_model(input_shape, num_layers, units, dropout_rate)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        contrastive_embeddings = contrastive_model(inputs)\n",
    "        regression_output = regression_model(inputs)\n",
    "\n",
    "        combined_input = Concatenate()([inputs[:,0,:], contrastive_embeddings, regression_output])\n",
    "\n",
    "        mid = Dense(12, activation='relu')(combined_input)\n",
    "\n",
    "        mid = Dense(4, activation='relu')(mid)\n",
    "\n",
    "        final_output = Dense(1, activation='sigmoid')(mid)\n",
    "\n",
    "        model = Model(inputs, final_output)\n",
    "        return model, contrastive_model, regression_model\n",
    "\n",
    "    # Example usage\n",
    "    input_shape = (60, 6)  # Update based on your actual data shape\n",
    "    num_layers = 2\n",
    "    num_units = 6\n",
    "    dense_units = 4\n",
    "\n",
    "    classification_model, contrastive_model, regression_model = combined_model(input_shape, num_layers, num_units, dense_units)\n",
    "    contrastive_model.summary()\n",
    "    regression_model.summary()\n",
    "    classification_model.summary()\n",
    "\n",
    "    # Function to generate triplets\n",
    "    def generate_triplets(X, y, num_samples=4, batch_size=64):\n",
    "        anchors = []\n",
    "        positives = []\n",
    "        negatives = []\n",
    "\n",
    "        num_batches = np.ceil(len(X) / batch_size).astype(int)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            anchor = X[i]\n",
    "            g = i // batch_size\n",
    "\n",
    "            # Calculate range for the current batch\n",
    "            batch_start = batch_size * g\n",
    "            batch_end = min(batch_size * (g + 1), len(X))\n",
    "\n",
    "            positive_indices = np.where((y == y[i]) & (np.arange(len(y)) >= batch_start) & (np.arange(len(y)) < batch_end))[0]\n",
    "            negative_indices = np.where((y != y[i]) & (np.arange(len(y)) >= batch_start) & (np.arange(len(y)) < batch_end))[0]\n",
    "\n",
    "            positive_indices = positive_indices[positive_indices != i]\n",
    "\n",
    "            selected_positives = list(positive_indices)\n",
    "            selected_negatives = list(negative_indices)\n",
    "\n",
    "            # Keep collecting positives and negatives until we have enough\n",
    "            batch = g + 1\n",
    "            while len(selected_positives) < num_samples or len(selected_negatives) < num_samples:\n",
    "                if batch >= num_batches:\n",
    "                    batch = 0  # Loop back to the start of the dataset\n",
    "\n",
    "                if len(selected_positives) < num_samples:\n",
    "                    batch_start = batch_size * batch\n",
    "                    batch_end = min(batch_size * (batch + 1), len(X))\n",
    "                    new_positives = np.where((y == y[i]) & (np.arange(len(y)) >= batch_start) & (np.arange(len(y)) < batch_end))[0]\n",
    "                    new_positives = new_positives[new_positives != i]\n",
    "                    selected_positives.extend(new_positives)\n",
    "\n",
    "                if len(selected_negatives) < num_samples:\n",
    "                    batch_start = batch_size * batch\n",
    "                    batch_end = min(batch_size * (batch + 1), len(X))\n",
    "                    new_negatives = np.where((y != y[i]) & (np.arange(len(y)) >= batch_start) & (np.arange(len(y)) < batch_end))[0]\n",
    "                    selected_negatives.extend(new_negatives)\n",
    "\n",
    "                if batch == g:\n",
    "                    # If we loop back to the original batch, break to avoid infinite loop\n",
    "                    break\n",
    "                batch += 1\n",
    "\n",
    "            selected_positives = np.array(selected_positives)[:num_samples]\n",
    "            selected_negatives = np.array(selected_negatives)[:num_samples]\n",
    "\n",
    "            anchors.append(anchor)\n",
    "            positives.append(X[selected_positives])\n",
    "            negatives.append(X[selected_negatives])\n",
    "\n",
    "        return np.array(anchors), np.array(positives), np.array(negatives)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "\n",
    "    anchors, positives, negatives = generate_triplets(X_train, y_train, 4, batch_size)\n",
    "\n",
    "    # Compile model with separate loss functions\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # Training loop with triplet and classification loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_triplet = 0\n",
    "        epoch_loss_regression = 0\n",
    "        epoch_loss_classification = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        for i in tqdm(range(0, len(anchors), batch_size), desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\"):\n",
    "            a_batch = anchors[i:i + batch_size]\n",
    "            p_batch = positives[i:i + batch_size]\n",
    "            n_batch = negatives[i:i + batch_size]\n",
    "            x_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            y_type_batch = y_type_train[i:i + batch_size]\n",
    "\n",
    "            # Reshape y_batch to match the shape of classification_output\n",
    "            y_batch = y_batch.reshape(-1, 1)\n",
    "            y_type_batch = y_type_batch.reshape(-1, 1)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                anchor_embeddings = contrastive_model(a_batch, training=True)\n",
    "\n",
    "                positive_embeddings_list = []\n",
    "                negative_embeddings_list = []\n",
    "\n",
    "                for j in range(4):\n",
    "                    positive_embedding = contrastive_model(p_batch[:, j, :, :], training=True)\n",
    "                    negative_embedding = contrastive_model(n_batch[:, j, :, :], training=True)\n",
    "                    positive_embeddings_list.append(positive_embedding)\n",
    "                    negative_embeddings_list.append(negative_embedding)\n",
    "\n",
    "                positive_embeddings = tf.stack(positive_embeddings_list, axis=1)\n",
    "                negative_embeddings = tf.stack(negative_embeddings_list, axis=1)\n",
    "\n",
    "                # Compute triplet loss\n",
    "                loss_triplet = triplet_loss(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "\n",
    "                # Compute regression loss\n",
    "                regression_output = regression_model(x_batch, training=True)\n",
    "                loss_regression = tf.keras.losses.mean_squared_error(y_type_batch, regression_output)\n",
    "                loss_regression = tf.reduce_mean(loss_regression) * 0.00000001\n",
    "\n",
    "                # Compute final classification loss\n",
    "                combined_output = classification_model(x_batch, training=True)\n",
    "                loss_classification = tf.keras.losses.binary_crossentropy(y_batch, combined_output)\n",
    "                loss_classification = tf.reduce_mean(loss_classification) \n",
    "\n",
    "                # Total loss\n",
    "                total_loss = loss_triplet + loss_regression + loss_classification\n",
    "\n",
    "            gradients = tape.gradient(total_loss, classification_model.trainable_variables + regression_model.trainable_variables + contrastive_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, classification_model.trainable_variables + regression_model.trainable_variables + contrastive_model.trainable_variables))\n",
    "\n",
    "            epoch_loss_triplet += loss_triplet.numpy()\n",
    "            epoch_loss_regression += loss_regression.numpy()\n",
    "            epoch_loss_classification += loss_classification.numpy()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss_triplet = epoch_loss_triplet / num_batches\n",
    "        avg_loss_regression = epoch_loss_regression / num_batches\n",
    "        avg_loss_classification = epoch_loss_classification / num_batches\n",
    "\n",
    "        print(f'Epoch {epoch + 1} - Triplet Loss: {avg_loss_triplet:.4f}, Regression Loss: {avg_loss_regression:.4f}, Classification Loss: {avg_loss_classification:.4f}')\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    \n",
    "    best_threshold = 0.0\n",
    "    best_tss = 0.0\n",
    "    y_pred = classification_model.predict(X_test)\n",
    "    # evaluate model\n",
    "    for i in range(1, 1000):\n",
    "\n",
    "        threshold = i / 1000 # Adjust the threshold as needed\n",
    "        y_pred_binary = (y_pred > threshold).astype(int)\n",
    "        confusion = confusion_matrix(y_test, y_pred_binary)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        tss = TSS(tp,tn,fp,fn)\n",
    "        if tss > best_tss:\n",
    "            best_tss = tss\n",
    "            best_threshold = i / 1000\n",
    "    \n",
    "    print(str(X_train.shape)+': The Classifier is Done! \\n')\n",
    "\n",
    "    \n",
    "    threshold = best_threshold # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    confusion = confusion_matrix(y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9485d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c9b60e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 60, 6)]           0         \n",
      "                                                                 \n",
      " gru_20 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " gru_21 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 6)                 0         \n",
      " 0 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532 (2.08 KB)\n",
      "Trainable params: 532 (2.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 60, 6)]           0         \n",
      "                                                                 \n",
      " gru_22 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " gru_23 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 6)                 0         \n",
      " 1 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 2)                 14        \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 521 (2.04 KB)\n",
      "Trainable params: 521 (2.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)       [(None, 60, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5  (None, 6)                    0         ['input_18[0][0]']            \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " model_15 (Functional)       (None, 4)                    532       ['input_18[0][0]']            \n",
      "                                                                                                  \n",
      " model_16 (Functional)       (None, 1)                    521       ['input_18[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 11)                   0         ['tf.__operators__.getitem_5[0\n",
      " )                                                                  ][0]',                        \n",
      "                                                                     'model_15[0][0]',            \n",
      "                                                                     'model_16[0][0]']            \n",
      "                                                                                                  \n",
      " dense_33 (Dense)            (None, 12)                   144       ['concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " dense_34 (Dense)            (None, 4)                    52        ['dense_33[0][0]']            \n",
      "                                                                                                  \n",
      " dense_35 (Dense)            (None, 1)                    5         ['dense_34[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1254 (4.90 KB)\n",
      "Trainable params: 1254 (4.90 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|█████████████████████████████| 78/78 [01:51<00:00,  1.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Triplet Loss: 1.2292, Regression Loss: 2.1041, Classification Loss: 0.4009\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|█████████████████████████████| 78/78 [01:49<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Triplet Loss: 0.9455, Regression Loss: 2.1041, Classification Loss: 0.1952\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|█████████████████████████████| 78/78 [01:50<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Triplet Loss: 0.8882, Regression Loss: 2.1041, Classification Loss: 0.1856\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|█████████████████████████████| 78/78 [01:49<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Triplet Loss: 0.8686, Regression Loss: 2.1041, Classification Loss: 0.1812\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|█████████████████████████████| 78/78 [01:50<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Triplet Loss: 0.8391, Regression Loss: 2.1041, Classification Loss: 0.1757\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|█████████████████████████████| 78/78 [01:50<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Triplet Loss: 0.8420, Regression Loss: 2.1041, Classification Loss: 0.1706\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|█████████████████████████████| 78/78 [01:49<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Triplet Loss: 0.8462, Regression Loss: 2.1041, Classification Loss: 0.1662\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|█████████████████████████████| 78/78 [01:50<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Triplet Loss: 0.8372, Regression Loss: 2.1041, Classification Loss: 0.1628\n",
      "Training completed!\n",
      "2741/2741 [==============================] - 5s 2ms/step\n",
      "(9981, 60, 6): The Classifier is Done! \n",
      "\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 60, 6)]           0         \n",
      "                                                                 \n",
      " gru_24 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " gru_25 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 6)                 0         \n",
      " 2 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532 (2.08 KB)\n",
      "Trainable params: 532 (2.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 60, 6)]           0         \n",
      "                                                                 \n",
      " gru_26 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " gru_27 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 6)                 0         \n",
      " 3 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 2)                 14        \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 521 (2.04 KB)\n",
      "Trainable params: 521 (2.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)       [(None, 60, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6  (None, 6)                    0         ['input_21[0][0]']            \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " model_18 (Functional)       (None, 4)                    532       ['input_21[0][0]']            \n",
      "                                                                                                  \n",
      " model_19 (Functional)       (None, 1)                    521       ['input_21[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 11)                   0         ['tf.__operators__.getitem_6[0\n",
      " )                                                                  ][0]',                        \n",
      "                                                                     'model_18[0][0]',            \n",
      "                                                                     'model_19[0][0]']            \n",
      "                                                                                                  \n",
      " dense_39 (Dense)            (None, 12)                   144       ['concatenate_6[0][0]']       \n",
      "                                                                                                  \n",
      " dense_40 (Dense)            (None, 4)                    52        ['dense_39[0][0]']            \n",
      "                                                                                                  \n",
      " dense_41 (Dense)            (None, 1)                    5         ['dense_40[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1254 (4.90 KB)\n",
      "Trainable params: 1254 (4.90 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|█████████████████████████████| 71/71 [01:40<00:00,  1.42s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Triplet Loss: 1.3362, Regression Loss: 0.1554, Classification Loss: 0.5540\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|█████████████████████████████| 71/71 [01:39<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Triplet Loss: 0.9068, Regression Loss: 0.1555, Classification Loss: 0.3293\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|█████████████████████████████| 71/71 [01:39<00:00,  1.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Triplet Loss: 0.9682, Regression Loss: 0.1555, Classification Loss: 0.1824\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|█████████████████████████████| 71/71 [01:42<00:00,  1.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Triplet Loss: 1.0910, Regression Loss: 0.1555, Classification Loss: 0.1577\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|█████████████████████████████| 71/71 [01:40<00:00,  1.42s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Triplet Loss: 1.2866, Regression Loss: 0.1556, Classification Loss: 0.1547\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|█████████████████████████████| 71/71 [01:45<00:00,  1.48s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Triplet Loss: 1.4492, Regression Loss: 0.1556, Classification Loss: 0.1533\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|█████████████████████████████| 71/71 [01:41<00:00,  1.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Triplet Loss: 1.2620, Regression Loss: 0.1556, Classification Loss: 0.1504\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|█████████████████████████████| 71/71 [01:43<00:00,  1.46s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Triplet Loss: 0.8686, Regression Loss: 0.1556, Classification Loss: 0.1484\n",
      "Training completed!\n",
      "1328/1328 [==============================] - 3s 2ms/step\n",
      "(9050, 60, 6): The Classifier is Done! \n",
      "\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 60, 6)]           0         \n",
      "                                                                 \n",
      " gru_28 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " gru_29 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 6)                 0         \n",
      " 4 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532 (2.08 KB)\n",
      "Trainable params: 532 (2.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 60, 6)]           0         \n",
      "                                                                 \n",
      " gru_30 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " gru_31 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 6)                 0         \n",
      " 5 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 2)                 14        \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 521 (2.04 KB)\n",
      "Trainable params: 521 (2.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)       [(None, 60, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7  (None, 6)                    0         ['input_24[0][0]']            \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " model_21 (Functional)       (None, 4)                    532       ['input_24[0][0]']            \n",
      "                                                                                                  \n",
      " model_22 (Functional)       (None, 1)                    521       ['input_24[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate  (None, 11)                   0         ['tf.__operators__.getitem_7[0\n",
      " )                                                                  ][0]',                        \n",
      "                                                                     'model_21[0][0]',            \n",
      "                                                                     'model_22[0][0]']            \n",
      "                                                                                                  \n",
      " dense_45 (Dense)            (None, 12)                   144       ['concatenate_7[0][0]']       \n",
      "                                                                                                  \n",
      " dense_46 (Dense)            (None, 4)                    52        ['dense_45[0][0]']            \n",
      "                                                                                                  \n",
      " dense_47 (Dense)            (None, 1)                    5         ['dense_46[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1254 (4.90 KB)\n",
      "Trainable params: 1254 (4.90 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|█████████████████████████████| 82/82 [01:58<00:00,  1.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Triplet Loss: 0.9605, Regression Loss: 0.4677, Classification Loss: 0.5518\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|█████████████████████████████| 82/82 [01:56<00:00,  1.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Triplet Loss: 1.0626, Regression Loss: 0.4678, Classification Loss: 0.4466\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|█████████████████████████████| 82/82 [01:57<00:00,  1.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Triplet Loss: 1.0529, Regression Loss: 0.4677, Classification Loss: 0.3652\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|█████████████████████████████| 82/82 [01:58<00:00,  1.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Triplet Loss: 0.9079, Regression Loss: 0.4677, Classification Loss: 0.2581\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|█████████████████████████████| 82/82 [02:00<00:00,  1.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Triplet Loss: 0.8718, Regression Loss: 0.4677, Classification Loss: 0.1852\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|█████████████████████████████| 82/82 [01:59<00:00,  1.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Triplet Loss: 0.8513, Regression Loss: 0.4677, Classification Loss: 0.1560\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|█████████████████████████████| 82/82 [01:58<00:00,  1.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Triplet Loss: 0.9471, Regression Loss: 0.4677, Classification Loss: 0.1416\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|█████████████████████████████| 82/82 [02:00<00:00,  1.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Triplet Loss: 0.8521, Regression Loss: 0.4677, Classification Loss: 0.1349\n",
      "Training completed!\n",
      "1601/1601 [==============================] - 3s 2ms/step\n",
      "(10375, 60, 6): The Classifier is Done! \n",
      "\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 60, 6)]           0         \n",
      "                                                                 \n",
      " gru_32 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " gru_33 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 6)                 0         \n",
      " 6 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532 (2.08 KB)\n",
      "Trainable params: 532 (2.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 60, 6)]           0         \n",
      "                                                                 \n",
      " gru_34 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " gru_35 (GRU)                (None, 60, 6)             252       \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 60, 6)             0         \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 6)                 0         \n",
      " 7 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 2)                 14        \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 521 (2.04 KB)\n",
      "Trainable params: 521 (2.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)       [(None, 60, 6)]              0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8  (None, 6)                    0         ['input_27[0][0]']            \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " model_24 (Functional)       (None, 4)                    532       ['input_27[0][0]']            \n",
      "                                                                                                  \n",
      " model_25 (Functional)       (None, 1)                    521       ['input_27[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 11)                   0         ['tf.__operators__.getitem_8[0\n",
      " )                                                                  ][0]',                        \n",
      "                                                                     'model_24[0][0]',            \n",
      "                                                                     'model_25[0][0]']            \n",
      "                                                                                                  \n",
      " dense_51 (Dense)            (None, 12)                   144       ['concatenate_8[0][0]']       \n",
      "                                                                                                  \n",
      " dense_52 (Dense)            (None, 4)                    52        ['dense_51[0][0]']            \n",
      "                                                                                                  \n",
      " dense_53 (Dense)            (None, 1)                    5         ['dense_52[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1254 (4.90 KB)\n",
      "Trainable params: 1254 (4.90 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|█████████████████████████████| 73/73 [01:44<00:00,  1.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Triplet Loss: 1.3809, Regression Loss: 0.6851, Classification Loss: 0.3184\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|█████████████████████████████| 73/73 [01:44<00:00,  1.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Triplet Loss: 0.7459, Regression Loss: 0.6852, Classification Loss: 0.1756\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|█████████████████████████████| 73/73 [01:44<00:00,  1.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Triplet Loss: 0.7929, Regression Loss: 0.6853, Classification Loss: 0.1175\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|█████████████████████████████| 73/73 [01:44<00:00,  1.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Triplet Loss: 0.7564, Regression Loss: 0.6852, Classification Loss: 0.1131\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|█████████████████████████████| 73/73 [01:44<00:00,  1.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Triplet Loss: 0.6910, Regression Loss: 0.6852, Classification Loss: 0.1096\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|█████████████████████████████| 73/73 [01:45<00:00,  1.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Triplet Loss: 0.6928, Regression Loss: 0.6852, Classification Loss: 0.1057\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|█████████████████████████████| 73/73 [01:45<00:00,  1.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Triplet Loss: 0.6898, Regression Loss: 0.6852, Classification Loss: 0.1016\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|█████████████████████████████| 73/73 [01:46<00:00,  1.46s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Triplet Loss: 0.6826, Regression Loss: 0.6852, Classification Loss: 0.0987\n",
      "Training completed!\n",
      "2353/2353 [==============================] - 5s 2ms/step\n",
      "(9268, 60, 6): The Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "contrastive_regression_result = kfold_training('ContrastiveRegression', data, labels, flare_type_labels_list, test_data, test_labels, test_flare_type_labels_list, contrastive_regression, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0ac96f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.8506630030092945    Recall: 0.9486081370449678\n",
      "TSS: 0.8260714426454    Recall: 0.988061797752809\n",
      "TSS: 0.8550793297744873    Recall: 0.9965665236051502\n",
      "TSS: 0.8530492674041973    Recall: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "save_results(contrastive_regression_result, \"contrastive_regression_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cbac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
