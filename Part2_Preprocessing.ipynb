{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074414f1",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be844b9",
   "metadata": {},
   "source": [
    "## Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "368c446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73492, 60, 6)\n",
      "Partition 1 flare_class_labels count: {1: 60130, 2: 6416, 3: 5692, 4: 1089, 5: 165}\n",
      "Partition 1 has NaN values: False\n",
      "(87684, 60, 6)\n",
      "Partition 2 flare_class_labels count: {1: 72498, 2: 8809, 3: 4976, 4: 1329, 5: 72}\n",
      "Partition 2 has NaN values: False\n",
      "(42482, 60, 6)\n",
      "Partition 3 flare_class_labels count: {1: 34734, 2: 5639, 3: 685, 4: 1288, 5: 136}\n",
      "Partition 3 has NaN values: False\n",
      "(51219, 60, 6)\n",
      "Partition 4 flare_class_labels count: {1: 43252, 2: 5956, 3: 846, 4: 1012, 5: 153}\n",
      "Partition 4 has NaN values: False\n",
      "(75292, 60, 6)\n",
      "Partition 5 flare_class_labels count: {1: 62615, 2: 5763, 3: 5924, 4: 971, 5: 19}\n",
      "Partition 5 has NaN values: False\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/1_Raw/\"\n",
    "label_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/2_Labels/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "raw_data = []\n",
    "labels = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load raw data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \".pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))\n",
    "\n",
    "# Load labels\n",
    "for i in range(num_partitions):\n",
    "    labels.append(pd.read_csv(label_dir + \"Partition\" + str(i+1) + \"_labels.csv\"))\n",
    "\n",
    "# Processing data and labels\n",
    "for i in range(num_partitions):\n",
    "    # Transpose data to (num_samples, num_timestamps, num_features) and remove the first feature\n",
    "    data = np.transpose(raw_data[i], (2, 0, 1))[:, :, 2:6]  # shape (num_samples, num_timestamps, 24 features)\n",
    "    \n",
    "    # Process FLARE_CLASS labels\n",
    "    class_mapping = {'X': 5, 'M': 4, 'B': 3, 'C': 2, 'FQ': 1}\n",
    "    flare_class_labels = labels[i]['FLARE_CLASS'].map(class_mapping).values\n",
    "\n",
    "    # Process FLARE_TYPE labels\n",
    "    type_mapping = {'FQ': 1, 'C': 10, 'B': 100, 'M': 1000, 'X': 10000}\n",
    "    \n",
    "    def calculate_flare_type(flare_type):\n",
    "        if flare_type == 'FQ':\n",
    "            return 1\n",
    "        else:\n",
    "            return type_mapping[flare_type[0]] * float(flare_type[1:])\n",
    "    \n",
    "    flare_type_labels = labels[i]['FLARE_TYPE'].apply(calculate_flare_type).values\n",
    "    \n",
    "    # Mean imputation and removal of invalid samples\n",
    "    valid_samples = []\n",
    "    for sample_idx in range(data.shape[0]):\n",
    "        valid_sample = True\n",
    "        for feature_idx in range(data.shape[2]):\n",
    "            feature_data = data[sample_idx, :, feature_idx]\n",
    "            n = len(feature_data)\n",
    "            valid_values = feature_data[(feature_data != 0) & (~np.isnan(feature_data))]\n",
    "            if len(valid_values) > 0:\n",
    "                for t in range(n):\n",
    "                    next_value_found = False\n",
    "                    if feature_data[t] == 0 or np.isnan(feature_data[t]):\n",
    "                        # Try to find the next available value\n",
    "                        for j in range(t + 1, n):\n",
    "                            if feature_data[j] != 0 and not np.isnan(feature_data[j]):\n",
    "                                feature_data[t] = feature_data[j]\n",
    "                                next_value_found = True\n",
    "                                break\n",
    "                    # If no next value is found, use the previous value\n",
    "                    if not next_value_found:\n",
    "                        for j in range(t - 1, -1, -1):\n",
    "                            if feature_data[j] != 0 and not np.isnan(feature_data[j]):\n",
    "                                feature_data[t] = feature_data[j]\n",
    "                                break\n",
    "    \n",
    "            else:\n",
    "                valid_sample = False\n",
    "                break  # Exit the loop if the sample is invalid\n",
    "            data[sample_idx, :, feature_idx] = feature_data\n",
    "        if valid_sample:\n",
    "            valid_samples.append(sample_idx)\n",
    "    \n",
    "    data = data[valid_samples]\n",
    "    flare_class_labels = flare_class_labels[valid_samples]\n",
    "    flare_type_labels = flare_type_labels[valid_samples]\n",
    "    print(data.shape)\n",
    "    \n",
    "    unique, counts = np.unique(flare_class_labels, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    print(f\"Partition {i+1} flare_class_labels count: {class_counts}\")\n",
    "    \n",
    "    # Check for any NaN values before saving\n",
    "    has_nan = np.isnan(data).any()\n",
    "    print(f\"Partition {i+1} has NaN values: {has_nan}\")\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_flare_class_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(flare_class_labels, f)\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_flare_type_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(flare_type_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22168451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47002, 60, 24)\n",
      "Partition 1 flare_class_labels count: {1: 33747, 2: 6375, 3: 5626, 4: 1089, 5: 165}\n",
      "Partition 1 has NaN values: False\n",
      "(53511, 60, 24)\n",
      "Partition 2 flare_class_labels count: {1: 38428, 2: 8775, 3: 4909, 4: 1327, 5: 72}\n",
      "Partition 2 has NaN values: False\n",
      "(25531, 60, 24)\n",
      "Partition 3 flare_class_labels count: {1: 17849, 2: 5583, 3: 675, 4: 1288, 5: 136}\n",
      "Partition 3 has NaN values: False\n",
      "(30167, 60, 24)\n",
      "Partition 4 flare_class_labels count: {1: 22234, 2: 5938, 3: 830, 4: 1012, 5: 153}\n",
      "Partition 4 has NaN values: False\n",
      "(42215, 60, 24)\n",
      "Partition 5 flare_class_labels count: {1: 29618, 2: 5738, 3: 5869, 4: 971, 5: 19}\n",
      "Partition 5 has NaN values: False\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/1_Raw/\"\n",
    "label_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/2_Labels/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "raw_data = []\n",
    "labels = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load raw data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \".pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))\n",
    "\n",
    "# Load labels\n",
    "for i in range(num_partitions):\n",
    "    labels.append(pd.read_csv(label_dir + \"Partition\" + str(i+1) + \"_labels.csv\"))\n",
    "\n",
    "# Processing data and labels\n",
    "for i in range(num_partitions):\n",
    "    # Transpose data to (num_samples, num_timestamps, num_features) and remove the first feature\n",
    "    data = np.transpose(raw_data[i], (2, 0, 1))[:, :, 1:]  # shape (num_samples, num_timestamps, 24 features)\n",
    "    \n",
    "    # Process FLARE_CLASS labels\n",
    "    class_mapping = {'X': 5, 'M': 4, 'B': 3, 'C': 2, 'FQ': 1}\n",
    "    flare_class_labels = labels[i]['FLARE_CLASS'].map(class_mapping).values\n",
    "\n",
    "    # Process FLARE_TYPE labels\n",
    "    type_mapping = {'FQ': 1, 'C': 10, 'B': 100, 'M': 1000, 'X': 10000}\n",
    "    \n",
    "    def calculate_flare_type(flare_type):\n",
    "        if flare_type == 'FQ':\n",
    "            return 1\n",
    "        else:\n",
    "            return type_mapping[flare_type[0]] * float(flare_type[1:])\n",
    "    \n",
    "    flare_type_labels = labels[i]['FLARE_TYPE'].apply(calculate_flare_type).values\n",
    "    \n",
    "    # Mean imputation and removal of invalid samples\n",
    "    valid_samples = []\n",
    "    for sample_idx in range(data.shape[0]):\n",
    "        valid_sample = True\n",
    "        for feature_idx in range(data.shape[2]):\n",
    "            feature_data = data[sample_idx, :, feature_idx]\n",
    "            n = len(feature_data)\n",
    "            valid_values = feature_data[(feature_data != 0) & (~np.isnan(feature_data))]\n",
    "            if len(valid_values) > 0:\n",
    "                for t in range(n):\n",
    "                    next_value_found = False\n",
    "                    if feature_data[t] == 0 or np.isnan(feature_data[t]):\n",
    "                        # Try to find the next available value\n",
    "                        for j in range(t + 1, n):\n",
    "                            if feature_data[j] != 0 and not np.isnan(feature_data[j]):\n",
    "                                feature_data[t] = feature_data[j]\n",
    "                                next_value_found = True\n",
    "                                break\n",
    "                    # If no next value is found, use the previous value\n",
    "                    if not next_value_found:\n",
    "                        for j in range(t - 1, -1, -1):\n",
    "                            if feature_data[j] != 0 and not np.isnan(feature_data[j]):\n",
    "                                feature_data[t] = feature_data[j]\n",
    "                                break\n",
    "    \n",
    "            else:\n",
    "                valid_sample = False\n",
    "                break  # Exit the loop if the sample is invalid\n",
    "            data[sample_idx, :, feature_idx] = feature_data\n",
    "        if valid_sample:\n",
    "            valid_samples.append(sample_idx)\n",
    "    \n",
    "    data = data[valid_samples]\n",
    "    flare_class_labels = flare_class_labels[valid_samples]\n",
    "    flare_type_labels = flare_type_labels[valid_samples]\n",
    "    print(data.shape)\n",
    "    \n",
    "    unique, counts = np.unique(flare_class_labels, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    print(f\"Partition {i+1} flare_class_labels count: {class_counts}\")\n",
    "    \n",
    "    # Check for any NaN values before saving\n",
    "    has_nan = np.isnan(data).any()\n",
    "    print(f\"Partition {i+1} has NaN values: {has_nan}\")\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_WFS_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_WFS_flare_class_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(flare_class_labels, f)\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_WFS_flare_type_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(flare_type_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35231d5",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "18a4b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1 has NaN values: False\n",
      "Partition 1 normalized data shape: (73492, 60, 6)\n",
      "Partition 1 binary labels distribution: [72238  1254]\n",
      "Partition 2 has NaN values: False\n",
      "Partition 2 normalized data shape: (87684, 60, 6)\n",
      "Partition 2 binary labels distribution: [86283  1401]\n",
      "Partition 3 has NaN values: False\n",
      "Partition 3 normalized data shape: (42482, 60, 6)\n",
      "Partition 3 binary labels distribution: [41058  1424]\n",
      "Partition 4 has NaN values: False\n",
      "Partition 4 normalized data shape: (51219, 60, 6)\n",
      "Partition 4 binary labels distribution: [50054  1165]\n",
      "Partition 5 has NaN values: False\n",
      "Partition 5 normalized data shape: (75292, 60, 6)\n",
      "Partition 5 binary labels distribution: [74302   990]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import skew, zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "raw_data = []\n",
    "raw_labels = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load processed data and labels\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_data.pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_flare_class_labels.pkl\", 'rb') as f:\n",
    "        raw_labels.append(pickle.load(f))\n",
    "\n",
    "# Function to apply normalization based on skewness\n",
    "def normalize_feature(feature_data):\n",
    "    feature_data_flat = feature_data.flatten()\n",
    "    feature_data_normalized = zscore(feature_data_flat)\n",
    "    \n",
    "    return feature_data_normalized.reshape(feature_data.shape)\n",
    "\n",
    "# Normalize data and convert labels to binary\n",
    "for i in range(num_partitions):\n",
    "    data = raw_data[i]\n",
    "    labels = raw_labels[i]\n",
    "    \n",
    "    num_samples, num_timestamps, num_features = data.shape\n",
    "\n",
    "    normalized_data = np.empty_like(data)\n",
    "\n",
    "    for feature_idx in range(num_features):\n",
    "        feature_data = data[:, :, feature_idx]\n",
    "        normalized_feature_data = normalize_feature(feature_data)\n",
    "        normalized_data[:, :, feature_idx] = normalized_feature_data\n",
    "\n",
    "    # Check for any NaN values before saving\n",
    "    has_nan = np.isnan(normalized_data).any()\n",
    "    print(f\"Partition {i+1} has NaN values: {has_nan}\")\n",
    "\n",
    "    # Convert labels to binary\n",
    "    binary_labels = np.where(np.isin(labels, [4, 5]), 1, 0)\n",
    "\n",
    "    # Save normalized data\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_normalized_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(normalized_data, f)\n",
    "    \n",
    "    # Save binary labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_binary_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(binary_labels, f)\n",
    "\n",
    "    print(f\"Partition {i+1} normalized data shape: {normalized_data.shape}\")\n",
    "    print(f\"Partition {i+1} binary labels distribution: {np.bincount(binary_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb81c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1 has NaN values: False\n",
      "Partition 1 normalized data shape: (47002, 60, 24)\n",
      "Partition 1 binary labels distribution: [45748  1254]\n",
      "Partition 2 has NaN values: False\n",
      "Partition 2 normalized data shape: (53511, 60, 24)\n",
      "Partition 2 binary labels distribution: [52112  1399]\n",
      "Partition 3 has NaN values: False\n",
      "Partition 3 normalized data shape: (25531, 60, 24)\n",
      "Partition 3 binary labels distribution: [24107  1424]\n",
      "Partition 4 has NaN values: False\n",
      "Partition 4 normalized data shape: (30167, 60, 24)\n",
      "Partition 4 binary labels distribution: [29002  1165]\n",
      "Partition 5 has NaN values: False\n",
      "Partition 5 normalized data shape: (42215, 60, 24)\n",
      "Partition 5 binary labels distribution: [41225   990]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import skew, zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "raw_data = []\n",
    "raw_labels = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load processed data and labels\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_data.pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_flare_class_labels.pkl\", 'rb') as f:\n",
    "        raw_labels.append(pickle.load(f))\n",
    "\n",
    "# Function to apply normalization based on skewness\n",
    "def normalize_feature(feature_data):\n",
    "    feature_data_flat = feature_data.flatten()\n",
    "    feature_data_normalized = zscore(feature_data_flat)\n",
    "    \n",
    "    return feature_data_normalized.reshape(feature_data.shape)\n",
    "\n",
    "# Normalize data and convert labels to binary\n",
    "for i in range(num_partitions):\n",
    "    data = raw_data[i]\n",
    "    labels = raw_labels[i]\n",
    "    \n",
    "    num_samples, num_timestamps, num_features = data.shape\n",
    "\n",
    "    normalized_data = np.empty_like(data)\n",
    "\n",
    "    for feature_idx in range(num_features):\n",
    "        feature_data = data[:, :, feature_idx]\n",
    "        normalized_feature_data = normalize_feature(feature_data)\n",
    "        normalized_data[:, :, feature_idx] = normalized_feature_data\n",
    "\n",
    "    # Check for any NaN values before saving\n",
    "    has_nan = np.isnan(normalized_data).any()\n",
    "    print(f\"Partition {i+1} has NaN values: {has_nan}\")\n",
    "\n",
    "    # Convert labels to binary\n",
    "    binary_labels = np.where(np.isin(labels, [4, 5]), 1, 0)\n",
    "\n",
    "    # Save normalized data\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_WFS_normalized_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(normalized_data, f)\n",
    "    \n",
    "    # Save binary labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_WFS_binary_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(binary_labels, f)\n",
    "\n",
    "    print(f\"Partition {i+1} normalized data shape: {normalized_data.shape}\")\n",
    "    print(f\"Partition {i+1} binary labels distribution: {np.bincount(binary_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66daef3b",
   "metadata": {},
   "source": [
    "## SMOTE Over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44949910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1 new data shape: (91496, 60, 24)\n",
      "Partition 1 new label distribution: [45748 45748]\n",
      "Partition 2 new data shape: (104224, 60, 24)\n",
      "Partition 2 new label distribution: [52112 52112]\n",
      "Partition 3 new data shape: (48214, 60, 24)\n",
      "Partition 3 new label distribution: [24107 24107]\n",
      "Partition 4 new data shape: (58004, 60, 24)\n",
      "Partition 4 new label distribution: [29002 29002]\n",
      "Partition 5 new data shape: (82450, 60, 24)\n",
      "Partition 5 new label distribution: [41225 41225]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "raw_data = []\n",
    "labels = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load normalized data and labels\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_normalized_data.pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_flare_class_labels.pkl\", 'rb') as f:\n",
    "        labels.append(pickle.load(f))\n",
    "\n",
    "# Convert classes for binary classification and apply SMOTE\n",
    "for i in range(num_partitions):\n",
    "    data = raw_data[i]\n",
    "    flare_class_labels = labels[i]\n",
    "    \n",
    "    # Convert classes\n",
    "    binary_labels = np.where(flare_class_labels >= 4, 1, 0)\n",
    "    \n",
    "    # Reshape data to (num_samples, num_timestamps * num_features) for SMOTE\n",
    "    num_samples, num_timestamps, num_features = data.shape\n",
    "    reshaped_data = data.reshape((num_samples, num_timestamps * num_features))\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE()\n",
    "    reshaped_data_smote, binary_labels_smote = smote.fit_resample(reshaped_data, binary_labels)\n",
    "    \n",
    "    # Reshape data back to (num_samples, num_timestamps, num_features)\n",
    "    new_data = reshaped_data_smote.reshape((-1, num_timestamps, num_features))\n",
    "    \n",
    "    # Save new data and labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_smote_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(new_data, f)\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_smote_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(binary_labels_smote, f)\n",
    "    \n",
    "    print(f\"Partition {i+1} new data shape: {new_data.shape}\")\n",
    "    print(f\"Partition {i+1} new label distribution: {np.bincount(binary_labels_smote)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3194b4",
   "metadata": {},
   "source": [
    "## Balanced Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d05ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1 normalized data shape: (14517, 60, 24)\n",
      "Partition 1 binary labels distribution: [9980 4537]\n",
      "Partition 2 normalized data shape: (13147, 60, 24)\n",
      "Partition 2 binary labels distribution: [9038 4109]\n",
      "Partition 3 normalized data shape: (13408, 60, 24)\n",
      "Partition 3 binary labels distribution: [8692 4716]\n",
      "Partition 4 normalized data shape: (12204, 60, 24)\n",
      "Partition 4 binary labels distribution: [7991 4213]\n",
      "Partition 5 normalized data shape: (8435, 60, 24)\n",
      "Partition 5 binary labels distribution: [5799 2636]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import skew, zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the paths\n",
    "data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "raw_data = []\n",
    "labels = []\n",
    "flare_type_labels_list = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load processed data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_normalized_data.pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_flare_class_labels.pkl\", 'rb') as f:\n",
    "        labels.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_flare_type_labels.pkl\", 'rb') as f:\n",
    "        flare_type_labels_list.append(pickle.load(f))\n",
    "\n",
    "# Gaussian Noise Injection\n",
    "def gaussian_noise_injection(data, num_samples, noise_proportion=0.1):\n",
    "    std_dev = np.std(data, axis=0)\n",
    "    noise_level = std_dev * noise_proportion\n",
    "\n",
    "    new_samples = []\n",
    "    for _ in range(num_samples):\n",
    "        sample_index = np.random.choice(len(data))\n",
    "        sample = data[sample_index]\n",
    "        noise = np.random.normal(0, noise_level, sample.shape)\n",
    "        new_sample = sample + noise\n",
    "        new_samples.append(new_sample)\n",
    "\n",
    "    return np.array(new_samples)\n",
    "\n",
    "\n",
    "# Process each partition\n",
    "for i in range(num_partitions):\n",
    "    data = raw_data[i]\n",
    "    flare_class_labels = labels[i]\n",
    "    flare_type_labels = flare_type_labels_list[i]\n",
    "\n",
    "    # Oversampling\n",
    "    augmented_data = []\n",
    "    augmented_class_labels = []\n",
    "    augmented_type_labels = []\n",
    "\n",
    "    for class_label, factor in [(5, 10), (4, 1.5)]:\n",
    "        class_indices = np.where(flare_class_labels == class_label)[0]\n",
    "        class_data = data[class_indices]\n",
    "        class_type_labels = flare_type_labels[class_indices]\n",
    "        num_samples = int(len(class_indices) * factor)\n",
    "\n",
    "        # Gaussian Noise Injection\n",
    "        gni_data = gaussian_noise_injection(class_data, num_samples)\n",
    "        gni_labels = np.full(num_samples, class_label)\n",
    "        gni_type_labels = np.random.choice(class_type_labels, num_samples, replace=True)\n",
    "        augmented_data.append(gni_data)\n",
    "        augmented_class_labels.append(gni_labels)\n",
    "        augmented_type_labels.append(gni_type_labels)\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    augmented_data = np.concatenate(augmented_data, axis=0)\n",
    "    augmented_class_labels = np.concatenate(augmented_class_labels, axis=0)\n",
    "    augmented_type_labels = np.concatenate(augmented_type_labels, axis=0)\n",
    "    data = np.concatenate((data, augmented_data), axis=0)\n",
    "    flare_class_labels = np.concatenate((flare_class_labels, augmented_class_labels), axis=0)\n",
    "    flare_type_labels = np.concatenate((flare_type_labels, augmented_type_labels), axis=0)\n",
    "\n",
    "    # Calculate target number of samples for minority classes\n",
    "    total_majority_class_samples = len(flare_class_labels[flare_class_labels == 5]) + len(flare_class_labels[flare_class_labels == 4])\n",
    "    keep_1 = int(total_majority_class_samples * 1.2)\n",
    "    keep_2_3 = int(total_majority_class_samples // 2)\n",
    "\n",
    "    # Undersample minority classes to match the target number of samples\n",
    "    minority_class_1_indices = np.where(flare_class_labels == 1)[0]\n",
    "    minority_class_2_indices = np.where(flare_class_labels == 2)[0]\n",
    "    minority_class_3_indices = np.where(flare_class_labels == 3)[0]\n",
    "\n",
    "    minority_class_1_samples_to_keep = np.random.choice(minority_class_1_indices, min(keep_1, len(minority_class_1_indices)), replace=False)\n",
    "    minority_class_2_samples_to_keep = np.random.choice(minority_class_2_indices, min(keep_2_3, len(minority_class_2_indices)), replace=False)\n",
    "    minority_class_3_samples_to_keep = np.random.choice(minority_class_3_indices, min(keep_2_3, len(minority_class_3_indices)), replace=False)\n",
    "\n",
    "    valid_indices = np.concatenate((minority_class_1_samples_to_keep, minority_class_2_samples_to_keep, minority_class_3_samples_to_keep, np.where(np.isin(flare_class_labels, [4, 5]))[0]))\n",
    "\n",
    "    data = data[valid_indices]\n",
    "    flare_class_labels = flare_class_labels[valid_indices]\n",
    "    flare_type_labels = flare_type_labels[valid_indices]\n",
    "\n",
    "    # Normalize data\n",
    "    binary_labels = np.where(flare_class_labels >= 4, 1, 0)\n",
    "    \n",
    "    \n",
    "    # Save normalized data and binary labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_OUS_normalized_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_OUS_binary_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(binary_labels, f)\n",
    "\n",
    "    # Save flare_type_labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_OUS_flare_type_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(flare_type_labels, f)\n",
    "\n",
    "    print(f\"Partition {i+1} normalized data shape: {data.shape}\")\n",
    "    print(f\"Partition {i+1} binary labels distribution: {np.bincount(binary_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ae716",
   "metadata": {},
   "source": [
    "##  Near Decision Boundary Sample Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9f8e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1 normalized data shape: (9981, 60, 24)\n",
      "Partition 1 binary labels distribution: [5444 4537]\n",
      "Partition 2 normalized data shape: (9039, 60, 24)\n",
      "Partition 2 binary labels distribution: [4930 4109]\n",
      "Partition 3 normalized data shape: (10375, 60, 24)\n",
      "Partition 3 binary labels distribution: [5659 4716]\n",
      "Partition 4 normalized data shape: (9268, 60, 24)\n",
      "Partition 4 binary labels distribution: [5055 4213]\n",
      "Partition 5 normalized data shape: (5799, 60, 24)\n",
      "Partition 5 binary labels distribution: [3163 2636]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import skew, zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Define the paths\n",
    "data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "raw_data = []\n",
    "labels = []\n",
    "flare_type_labels_list = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load processed data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_normalized_data.pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_flare_class_labels.pkl\", 'rb') as f:\n",
    "        labels.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_WFS_flare_type_labels.pkl\", 'rb') as f:\n",
    "        flare_type_labels_list.append(pickle.load(f))\n",
    "\n",
    "# Gaussian Noise Injection\n",
    "def smote_synthetic_samples(data, num_samples, k_neighbors=5):\n",
    "    n_samples, n_timestamps, n_features = data.shape\n",
    "    \n",
    "    # Reshape the data for Nearest Neighbors to work on a 2D array\n",
    "    reshaped_data = data.reshape(n_samples, -1)\n",
    "    \n",
    "    # Nearest Neighbors to determine the points for interpolation\n",
    "    nn = NearestNeighbors(n_neighbors=k_neighbors+1)\n",
    "    nn.fit(reshaped_data)\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly pick an index\n",
    "        sample_index = np.random.randint(0, n_samples)\n",
    "        sample = data[sample_index]\n",
    "        \n",
    "        # Find k-nearest neighbors\n",
    "        neighbors = nn.kneighbors([reshaped_data[sample_index]], return_distance=False)[0]\n",
    "        # Exclude the sample itself\n",
    "        neighbors = neighbors[neighbors != sample_index]\n",
    "        \n",
    "        # Randomly select one of the neighbors\n",
    "        neighbor_index = np.random.choice(neighbors)\n",
    "        neighbor = data[neighbor_index]\n",
    "        \n",
    "        # Generate a synthetic sample\n",
    "        diff = neighbor - sample\n",
    "        gap = np.random.rand()\n",
    "        synthetic_sample = sample + gap * diff\n",
    "        synthetic_samples.append(synthetic_sample)\n",
    "    \n",
    "    return np.array(synthetic_samples)\n",
    "\n",
    "# Process each partition\n",
    "for i in range(num_partitions):\n",
    "    data = raw_data[i]\n",
    "    flare_class_labels = labels[i]\n",
    "    flare_type_labels = flare_type_labels_list[i]\n",
    "\n",
    "    # Oversampling\n",
    "    augmented_data = []\n",
    "    augmented_class_labels = []\n",
    "    augmented_type_labels = []\n",
    "\n",
    "    for class_label, factor in [(5, 10), (4, 1.5)]:\n",
    "        class_indices = np.where(flare_class_labels == class_label)[0]\n",
    "        class_data = data[class_indices]\n",
    "        class_type_labels = flare_type_labels[class_indices]\n",
    "        num_samples = int(len(class_indices) * factor)\n",
    "\n",
    "        # Gaussian Noise Injection\n",
    "        gni_data = smote_synthetic_samples(class_data, num_samples)\n",
    "        gni_labels = np.full(num_samples, class_label)\n",
    "        gni_type_labels = np.random.choice(class_type_labels, num_samples, replace=True)\n",
    "        augmented_data.append(gni_data)\n",
    "        augmented_class_labels.append(gni_labels)\n",
    "        augmented_type_labels.append(gni_type_labels)\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    augmented_data = np.concatenate(augmented_data, axis=0)\n",
    "    augmented_class_labels = np.concatenate(augmented_class_labels, axis=0)\n",
    "    augmented_type_labels = np.concatenate(augmented_type_labels, axis=0)\n",
    "    data = np.concatenate((data, augmented_data), axis=0)\n",
    "    flare_class_labels = np.concatenate((flare_class_labels, augmented_class_labels), axis=0)\n",
    "    flare_type_labels = np.concatenate((flare_type_labels, augmented_type_labels), axis=0)\n",
    "\n",
    "    # Remove class 2 and 3 samples\n",
    "    valid_indices = np.where((flare_class_labels != 2) & (flare_class_labels != 3))[0]\n",
    "    data = data[valid_indices]\n",
    "    flare_class_labels = flare_class_labels[valid_indices]\n",
    "    flare_type_labels = flare_type_labels[valid_indices]\n",
    "\n",
    "    # Calculate target number of samples for minority class 1\n",
    "    total_majority_class_samples = len(flare_class_labels[flare_class_labels == 5]) + len(flare_class_labels[flare_class_labels == 4])\n",
    "    keep_1 = int(total_majority_class_samples * 1.2)\n",
    "\n",
    "    # Undersample minority class 1 to match the target number of samples\n",
    "    minority_class_1_indices = np.where(flare_class_labels == 1)[0]\n",
    "    minority_class_1_samples_to_keep = np.random.choice(minority_class_1_indices, min(keep_1, len(minority_class_1_indices)), replace=False)\n",
    "\n",
    "    valid_indices = np.concatenate((minority_class_1_samples_to_keep, np.where(np.isin(flare_class_labels, [4, 5]))[0]))\n",
    "\n",
    "    data = data[valid_indices]\n",
    "    flare_class_labels = flare_class_labels[valid_indices]\n",
    "    flare_type_labels = flare_type_labels[valid_indices]\n",
    "\n",
    "    # Update binary labels: classes 5 and 4 as 1, class 1 as 0\n",
    "    binary_labels = np.where(np.isin(flare_class_labels, [4, 5]), 1, 0)\n",
    "\n",
    "    # Shuffle the data and labels\n",
    "    indices = np.random.permutation(len(data))\n",
    "    data = data[indices]\n",
    "    binary_labels = binary_labels[indices]\n",
    "    flare_type_labels = flare_type_labels[indices]\n",
    "\n",
    "    # Save normalized data and binary labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_CCBR_OUS_normalized_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_CCBR_OUS_binary_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(binary_labels, f)\n",
    "\n",
    "    # Save flare_type_labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_CCBR_OUS_flare_type_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(flare_type_labels, f)\n",
    "\n",
    "    print(f\"Partition {i+1} normalized data shape: {data.shape}\")\n",
    "    print(f\"Partition {i+1} binary labels distribution: {np.bincount(binary_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f0646",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db616ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 1 normalized data shape: (9981, 60, 6)\n",
      "Partition 1 binary labels distribution: [5444 4537]\n",
      "Partition 2 normalized data shape: (9050, 60, 6)\n",
      "Partition 2 binary labels distribution: [4936 4114]\n",
      "Partition 3 normalized data shape: (10375, 60, 6)\n",
      "Partition 3 binary labels distribution: [5659 4716]\n",
      "Partition 4 normalized data shape: (9268, 60, 6)\n",
      "Partition 4 binary labels distribution: [5055 4213]\n",
      "Partition 5 normalized data shape: (5799, 60, 6)\n",
      "Partition 5 binary labels distribution: [3163 2636]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import skew, zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Define the paths\n",
    "data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "raw_data = []\n",
    "labels = []\n",
    "flare_type_labels_list = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load processed data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_normalized_data.pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_flare_class_labels.pkl\", 'rb') as f:\n",
    "        labels.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_flare_type_labels.pkl\", 'rb') as f:\n",
    "        flare_type_labels_list.append(pickle.load(f))\n",
    "\n",
    "# Gaussian Noise Injection\n",
    "def smote_synthetic_samples(data, num_samples, k_neighbors=5):\n",
    "    n_samples, n_timestamps, n_features = data.shape\n",
    "    \n",
    "    # Reshape the data for Nearest Neighbors to work on a 2D array\n",
    "    reshaped_data = data.reshape(n_samples, -1)\n",
    "    \n",
    "    # Nearest Neighbors to determine the points for interpolation\n",
    "    nn = NearestNeighbors(n_neighbors=k_neighbors+1)\n",
    "    nn.fit(reshaped_data)\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly pick an index\n",
    "        sample_index = np.random.randint(0, n_samples)\n",
    "        sample = data[sample_index]\n",
    "        \n",
    "        # Find k-nearest neighbors\n",
    "        neighbors = nn.kneighbors([reshaped_data[sample_index]], return_distance=False)[0]\n",
    "        # Exclude the sample itself\n",
    "        neighbors = neighbors[neighbors != sample_index]\n",
    "        \n",
    "        # Randomly select one of the neighbors\n",
    "        neighbor_index = np.random.choice(neighbors)\n",
    "        neighbor = data[neighbor_index]\n",
    "        \n",
    "        # Generate a synthetic sample\n",
    "        diff = neighbor - sample\n",
    "        gap = np.random.rand()\n",
    "        synthetic_sample = sample + gap * diff\n",
    "        synthetic_samples.append(synthetic_sample)\n",
    "    \n",
    "    return np.array(synthetic_samples)\n",
    "\n",
    "# Process each partition\n",
    "for i in range(num_partitions):\n",
    "    data = raw_data[i]\n",
    "    flare_class_labels = labels[i]\n",
    "    flare_type_labels = flare_type_labels_list[i]\n",
    "\n",
    "    # Oversampling\n",
    "    augmented_data = []\n",
    "    augmented_class_labels = []\n",
    "    augmented_type_labels = []\n",
    "\n",
    "    for class_label, factor in [(5, 10), (4, 1.5)]:\n",
    "        class_indices = np.where(flare_class_labels == class_label)[0]\n",
    "        class_data = data[class_indices]\n",
    "        class_type_labels = flare_type_labels[class_indices]\n",
    "        num_samples = int(len(class_indices) * factor)\n",
    "\n",
    "        # Gaussian Noise Injection\n",
    "        gni_data = smote_synthetic_samples(class_data, num_samples)\n",
    "        gni_labels = np.full(num_samples, class_label)\n",
    "        gni_type_labels = np.random.choice(class_type_labels, num_samples, replace=True)\n",
    "        augmented_data.append(gni_data)\n",
    "        augmented_class_labels.append(gni_labels)\n",
    "        augmented_type_labels.append(gni_type_labels)\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    augmented_data = np.concatenate(augmented_data, axis=0)\n",
    "    augmented_class_labels = np.concatenate(augmented_class_labels, axis=0)\n",
    "    augmented_type_labels = np.concatenate(augmented_type_labels, axis=0)\n",
    "    data = np.concatenate((data, augmented_data), axis=0)\n",
    "    flare_class_labels = np.concatenate((flare_class_labels, augmented_class_labels), axis=0)\n",
    "    flare_type_labels = np.concatenate((flare_type_labels, augmented_type_labels), axis=0)\n",
    "\n",
    "    # Remove class 2 and 3 samples\n",
    "    valid_indices = np.where((flare_class_labels != 2) & (flare_class_labels != 3))[0]\n",
    "    data = data[valid_indices]\n",
    "    flare_class_labels = flare_class_labels[valid_indices]\n",
    "    flare_type_labels = flare_type_labels[valid_indices]\n",
    "\n",
    "    # Calculate target number of samples for minority class 1\n",
    "    total_majority_class_samples = len(flare_class_labels[flare_class_labels == 5]) + len(flare_class_labels[flare_class_labels == 4])\n",
    "    keep_1 = int(total_majority_class_samples * 1.2)\n",
    "\n",
    "    # Undersample minority class 1 to match the target number of samples\n",
    "    minority_class_1_indices = np.where(flare_class_labels == 1)[0]\n",
    "    minority_class_1_samples_to_keep = np.random.choice(minority_class_1_indices, min(keep_1, len(minority_class_1_indices)), replace=False)\n",
    "\n",
    "    valid_indices = np.concatenate((minority_class_1_samples_to_keep, np.where(np.isin(flare_class_labels, [4, 5]))[0]))\n",
    "\n",
    "    data = data[valid_indices]\n",
    "    flare_class_labels = flare_class_labels[valid_indices]\n",
    "    flare_type_labels = flare_type_labels[valid_indices]\n",
    "\n",
    "    # Update binary labels: classes 5 and 4 as 1, class 1 as 0\n",
    "    binary_labels = np.where(np.isin(flare_class_labels, [4, 5]), 1, 0)\n",
    "\n",
    "    # Shuffle the data and labels\n",
    "    indices = np.random.permutation(len(data))\n",
    "    data = data[indices]\n",
    "    binary_labels = binary_labels[indices]\n",
    "    flare_type_labels = flare_type_labels[indices]\n",
    "\n",
    "    # Save normalized data and binary labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_normalized_data.pkl\", 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_binary_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(binary_labels, f)\n",
    "\n",
    "    # Save flare_type_labels\n",
    "    with open(processed_data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_flare_type_labels.pkl\", 'wb') as f:\n",
    "        pickle.dump(flare_type_labels, f)\n",
    "\n",
    "    print(f\"Partition {i+1} normalized data shape: {data.shape}\")\n",
    "    print(f\"Partition {i+1} binary labels distribution: {np.bincount(binary_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a0b0d",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e41febc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def TSS(TP,TN,FP,FN):\n",
    "    TSS_value = (TP / (TP + FN)) - (FP / (FP + TN))\n",
    "    return TSS_value\n",
    "\n",
    "def HSS1(TP,TN,FP,FN):\n",
    "    HSS1_value = (2 * (TP * TN - FP * FN)) / ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN))\n",
    "    return HSS1_value\n",
    "    \n",
    "def HSS2(TP,TN,FP,FN):\n",
    "    HSS2_value = (2 * (TP * TN - FP * FN)) / ((TP + FP) * (FN + TN) + (TP + FN) * (FP + TN))\n",
    "    return HSS2_value\n",
    "\n",
    "def GSS(TP,TN,FP,FN):\n",
    "    GSS_value = (TP - (TP + FP) * (TP + FN) / (TP + FP + FN + TN))\n",
    "    return GSS_value\n",
    "\n",
    "def Recall(TP,TN,FP,FN):\n",
    "    Recall_value = (TP) / (TP + FN)\n",
    "    return Recall_value\n",
    "\n",
    "def FPR(TP,TN,FP,FN):\n",
    "    fpr_value = (FP) / (FP + TN)\n",
    "    return fpr_value\n",
    "\n",
    "def Accuracy(TP,TN,FP,FN):\n",
    "    accuracy_value = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return accuracy_value\n",
    "\n",
    "def Precision(TP,TN,FP,FN):\n",
    "    precision_value = (TP) / (TP + FP)\n",
    "    return precision_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ce491597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_training(name, X_train, Y_train, X_test, Y_test, training_func, num):\n",
    "    kfold = np.array([[1,2],[2,3],[3,4],[4,5]])\n",
    "\n",
    "    metrics = []\n",
    "    metrics_values = np.array([])\n",
    "    \n",
    "    for i in range(0, num):\n",
    "        train_index = kfold[i,0]\n",
    "        test_index = kfold[i,1]\n",
    "        metrics_values = training_func(X_train[train_index-1], Y_train[train_index-1], X_test[test_index-1], Y_test[test_index-1])\n",
    "        while (metrics_values[4] < 0.01):\n",
    "            metrics_values = training_func(X_train[train_index-1], Y_train[train_index-1], X_test[test_index-1], Y_test[test_index-1])\n",
    "        metrics.append(np.append(np.append(train_index, test_index), metrics_values))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9d359729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def gru_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 10, 64\n",
    "    n_timesteps, n_features = 60, 6\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=8, activation='tanh', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.98))\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    best_threshold = 0.0\n",
    "    best_tss = 0.0\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate model\n",
    "    for i in range(1, 10):\n",
    "\n",
    "        threshold = i / 10 # Adjust the threshold as needed\n",
    "        y_pred_binary = (y_pred > threshold).astype(int)\n",
    "        confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        tss = TSS(tp,tn,fp,fn)\n",
    "        if tss > best_tss:\n",
    "            best_tss = tss\n",
    "            best_threshold = i / 10\n",
    "        \n",
    "    \n",
    "    print(str(X_train.shape)+': GRU Classifier is Done! \\n')\n",
    "\n",
    "    \n",
    "    threshold = best_threshold # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "307b38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def rnn_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 10, 64\n",
    "    n_timesteps, n_features = 60, 6\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=8, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.98))\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "    best_threshold = 0.0\n",
    "    best_tss = 0.0\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate model\n",
    "    for i in range(1, 10):\n",
    "\n",
    "        threshold = i / 10 # Adjust the threshold as needed\n",
    "        y_pred_binary = (y_pred > threshold).astype(int)\n",
    "        confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        tss = TSS(tp,tn,fp,fn)\n",
    "        if tss > best_tss:\n",
    "            best_tss = tss\n",
    "            best_threshold = i / 10\n",
    "        \n",
    "    \n",
    "    print(str(X_train.shape)+': RNN Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    threshold = best_threshold # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9abbb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cnn_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 10, 64\n",
    "    n_timesteps, n_features = 60, 6\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.98))\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    best_threshold = 0.0\n",
    "    best_tss = 0.0\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate model\n",
    "    for i in range(1, 10):\n",
    "\n",
    "        threshold = i / 10 # Adjust the threshold as needed\n",
    "        y_pred_binary = (y_pred > threshold).astype(int)\n",
    "        confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        tss = TSS(tp,tn,fp,fn)\n",
    "        if tss > best_tss:\n",
    "            best_tss = tss\n",
    "            best_threshold = i / 10\n",
    "        \n",
    "    \n",
    "    print(str(X_train.shape)+': CNN Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    threshold = best_threshold # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0597fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def svm_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "\n",
    "\n",
    "    # Create an SVM classifier (you can choose different kernels like 'linear', 'rbf', etc.)\n",
    "    svm_classifier = SVC(kernel='rbf', C=1.0)\n",
    "    svm_classifier.fit(X_train[:,1,:], Y_train)\n",
    "    y_pred = svm_classifier.predict(X_test[:,1,:])\n",
    "    \n",
    "    \n",
    "    print(str(X_train.shape)+': SVM Classifier is Done! \\n')\n",
    "    \n",
    "\n",
    "    confusion = confusion_matrix(Y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"svm_model.pkl\")\n",
    "\n",
    "    #loaded_svm_model = joblib.load(data_dir + \"svm_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c6471b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def lstm_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 10, 64\n",
    "    n_timesteps, n_features = 60, 6\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(8, activation='tanh', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.98))\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    best_threshold = 0.0\n",
    "    best_tss = 0.0\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate model\n",
    "    for i in range(1, 10):\n",
    "\n",
    "        threshold = i / 10 # Adjust the threshold as needed\n",
    "        y_pred_binary = (y_pred > threshold).astype(int)\n",
    "        confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        tss = TSS(tp,tn,fp,fn)\n",
    "        if tss > best_tss:\n",
    "            best_tss = tss\n",
    "            best_threshold = i / 10\n",
    "        \n",
    "    \n",
    "    print(str(X_train.shape)+': LSTM Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    threshold = best_threshold # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "66b786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def lstm_WFS_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 10, 64\n",
    "    n_timesteps, n_features = 60, 24\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(24, activation='tanh', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.98))\n",
    "    \n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    best_threshold = 0.0\n",
    "    best_tss = 0.0\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate model\n",
    "    for i in range(1, 1000):\n",
    "\n",
    "        threshold = i / 1000 # Adjust the threshold as needed\n",
    "        y_pred_binary = (y_pred > threshold).astype(int)\n",
    "        confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        tss = TSS(tp,tn,fp,fn)\n",
    "        if tss > best_tss:\n",
    "            best_tss = tss\n",
    "            best_threshold = i / 1000\n",
    "        lstm_WFS_model\n",
    "    \n",
    "    print(str(X_train.shape)+': LSTM Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    threshold = best_threshold # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "58e80bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(result, name):\n",
    "    data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/results/\"\n",
    "\n",
    "    with open(data_dir + name + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    for i in range(4):\n",
    "        print(\"TSS: \" + str(result[i][6]) + \"    Recall: \" + str(result[i][10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "37cc2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "processed_data_dir = \"/Users/samskanderi/MLP-ContrastiveLR-SWANSF/I_Data/\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "data = []\n",
    "labels = []\n",
    "flare_type_labels_list = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "# Load processed data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_normalized_data.pkl\", 'rb') as f:\n",
    "        data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_binary_labels.pkl\", 'rb') as f:\n",
    "        labels.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_FS_CCBR_OUS_flare_type_labels.pkl\", 'rb') as f:\n",
    "        flare_type_labels_list.append(pickle.load(f))\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "test_flare_type_labels_list = []\n",
    "\n",
    "# Load processed data\n",
    "for i in range(num_partitions):\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_normalized_data.pkl\", 'rb') as f:\n",
    "        test_data.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_binary_labels.pkl\", 'rb') as f:\n",
    "        test_labels.append(pickle.load(f))\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_flare_type_labels.pkl\", 'rb') as f:\n",
    "        test_flare_type_labels_list.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb1bba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1673/1673 [==============================] - 4s 2ms/step\n",
      "(47002, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "798/798 [==============================] - 2s 2ms/step\n",
      "(53511, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "943/943 [==============================] - 2s 2ms/step\n",
      "(25531, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "1320/1320 [==============================] - 4s 3ms/step\n",
      "(30167, 60, 24): LSTM Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_data = kfold_training('LSTM', data, labels, test_data, test_labels, lstm_WFS_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e49c6f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.09508594289574356    Recall: 0.17512508934953538\n",
      "TSS: 0.1479381717185041    Recall: 0.28370786516853935\n",
      "TSS: 0.22318869232934357    Recall: 0.9356223175965666\n",
      "TSS: 0.22674642115515373    Recall: 0.41515151515151516\n"
     ]
    }
   ],
   "source": [
    "save_results(lstm_data, \"lstm_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45233d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "939de014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1673/1673 [==============================] - 4s 2ms/step\n",
      "(47002, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "798/798 [==============================] - 2s 2ms/step\n",
      "(53511, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "943/943 [==============================] - 2s 2ms/step\n",
      "(25531, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "1320/1320 [==============================] - 3s 2ms/step\n",
      "(30167, 60, 24): LSTM Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_normalized = kfold_training('LSTM', data, labels, test_data, test_labels, lstm_WFS_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd8630d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.6000649917053344    Recall: 0.8842030021443888\n",
      "TSS: 0.5971610127227721    Recall: 0.764747191011236\n",
      "TSS: 0.6985251276144046    Recall: 0.8729613733905579\n",
      "TSS: 0.7259134706678673    Recall: 0.902020202020202\n"
     ]
    }
   ],
   "source": [
    "save_results(lstm_normalized, \"lstm_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f94441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "671efb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1673/1673 [==============================] - 4s 2ms/step\n",
      "(91496, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "798/798 [==============================] - 2s 2ms/step\n",
      "(104224, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "943/943 [==============================] - 2s 2ms/step\n",
      "(48214, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "1320/1320 [==============================] - 3s 2ms/step\n",
      "(58004, 60, 24): LSTM Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_smote_normalized = kfold_training('LSTM', data, labels, test_data, test_labels, lstm_WFS_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5bfaa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.5624997393857579    Recall: 0.6847748391708363\n",
      "TSS: 0.5879832388186936    Recall: 0.8167134831460674\n",
      "TSS: 0.7153310131342133    Recall: 0.9184549356223176\n",
      "TSS: 0.5796492517656859    Recall: 0.7717171717171717\n"
     ]
    }
   ],
   "source": [
    "save_results(lstm_smote_normalized, \"lstm_smote_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a5464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "293a9adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1673/1673 [==============================] - 4s 2ms/step\n",
      "(14517, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "798/798 [==============================] - 2s 2ms/step\n",
      "(13147, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "943/943 [==============================] - 3s 3ms/step\n",
      "(13408, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "1320/1320 [==============================] - 3s 2ms/step\n",
      "(12204, 60, 24): LSTM Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_OUS_normalized = kfold_training('LSTM', data, labels, test_data, test_labels, lstm_WFS_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "746b7303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.6463976226055587    Recall: 0.9027877055039314\n",
      "TSS: 0.5352514864674022    Recall: 0.6306179775280899\n",
      "TSS: 0.6369930977085196    Recall: 0.9424892703862661\n",
      "TSS: 0.7385911265474638    Recall: 0.9565656565656566\n"
     ]
    }
   ],
   "source": [
    "save_results(lstm_OUS_normalized, \"lstm_OUS_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e86bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c04a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1673/1673 [==============================] - 4s 2ms/step\n",
      "(9981, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "798/798 [==============================] - 2s 2ms/step\n",
      "(9039, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "943/943 [==============================] - 2s 2ms/step\n",
      "(10375, 60, 24): LSTM Classifier is Done! \n",
      "\n",
      "1320/1320 [==============================] - 3s 2ms/step\n",
      "(9268, 60, 24): LSTM Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_CCBR_OUS_normalized = kfold_training('LSTM', data, labels, test_data, test_labels, lstm_WFS_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a95693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.6456352024989119    Recall: 0.8777698355968548\n",
      "TSS: 0.6365094897607716    Recall: 0.8679775280898876\n",
      "TSS: 0.7346839184984431    Recall: 0.8918454935622318\n",
      "TSS: 0.7447808589227631    Recall: 0.9363636363636364\n"
     ]
    }
   ],
   "source": [
    "save_results(lstm_CCBR_OUS_normalized, \"lstm_CCBR_OUS_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d222000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dc80d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2741/2741 [==============================] - 3s 1ms/step\n",
      "(9981, 60, 6): LSTM Classifier is Done! \n",
      "\n",
      "1328/1328 [==============================] - 1s 987us/step\n",
      "(9050, 60, 6): LSTM Classifier is Done! \n",
      "\n",
      "1601/1601 [==============================] - 2s 973us/step\n",
      "(10375, 60, 6): LSTM Classifier is Done! \n",
      "\n",
      "2353/2353 [==============================] - 2s 984us/step\n",
      "(9268, 60, 6): LSTM Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_FS_CCBR_OUS_normalized = kfold_training('LSTM', data, labels, test_data, test_labels, lstm_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ad3fb106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.7859160536932386    Recall: 0.9357601713062098\n",
      "TSS: 0.7616592737267806    Recall: 0.9403089887640449\n",
      "TSS: 0.8248079198928677    Recall: 0.9527896995708155\n",
      "TSS: 0.8297956823218593    Recall: 0.9515151515151515\n"
     ]
    }
   ],
   "source": [
    "save_results(lstm_FS_CCBR_OUS_normalized, \"lstm_FS_CCBR_OUS_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1df80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "46881786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2741/2741 [==============================] - 3s 994us/step\n",
      "(9981, 60, 6): GRU Classifier is Done! \n",
      "\n",
      "1328/1328 [==============================] - 1s 1000us/step\n",
      "(9050, 60, 6): GRU Classifier is Done! \n",
      "\n",
      "1601/1601 [==============================] - 2s 993us/step\n",
      "(10375, 60, 6): GRU Classifier is Done! \n",
      "\n",
      "2353/2353 [==============================] - 2s 1ms/step\n",
      "(9268, 60, 6): GRU Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gru_FS_CCBR_OUS_normalized = kfold_training('GRU', data, labels, test_data, test_labels, gru_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5c5d5499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.7967563340008494    Recall: 0.9179157744468237\n",
      "TSS: 0.7554087982415667    Recall: 0.8883426966292135\n",
      "TSS: 0.8321155126712079    Recall: 0.9339055793991416\n",
      "TSS: 0.8358704538861197    Recall: 0.9585858585858585\n"
     ]
    }
   ],
   "source": [
    "save_results(gru_FS_CCBR_OUS_normalized, \"gru_FS_CCBR_OUS_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ac099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d4b98d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2741/2741 [==============================] - 1s 391us/step\n",
      "(9981, 60, 6): CNN Classifier is Done! \n",
      "\n",
      "1328/1328 [==============================] - 1s 392us/step\n",
      "(9050, 60, 6): CNN Classifier is Done! \n",
      "\n",
      "1601/1601 [==============================] - 1s 393us/step\n",
      "(10375, 60, 6): CNN Classifier is Done! \n",
      "\n",
      "2353/2353 [==============================] - 1s 396us/step\n",
      "(9268, 60, 6): CNN Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_FS_CCBR_OUS_normalized = kfold_training('CNN', data, labels, test_data, test_labels, cnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bc129134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.7223450563966328    Recall: 0.8979300499643112\n",
      "TSS: 0.6988357385359488    Recall: 0.8033707865168539\n",
      "TSS: 0.8238489555743317    Recall: 0.9527896995708155\n",
      "TSS: 0.8028013439011797    Recall: 0.9181818181818182\n"
     ]
    }
   ],
   "source": [
    "save_results(cnn_FS_CCBR_OUS_normalized, \"cnn_FS_CCBR_OUS_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c150e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a2fa54ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2741/2741 [==============================] - 2s 640us/step\n",
      "(9981, 60, 6): RNN Classifier is Done! \n",
      "\n",
      "1328/1328 [==============================] - 1s 653us/step\n",
      "(9050, 60, 6): RNN Classifier is Done! \n",
      "\n",
      "1601/1601 [==============================] - 1s 656us/step\n",
      "(10375, 60, 6): RNN Classifier is Done! \n",
      "\n",
      "2353/2353 [==============================] - 2s 654us/step\n",
      "(9268, 60, 6): RNN Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_FS_CCBR_OUS_normalized = kfold_training('RNN', data, labels, test_data, test_labels, rnn_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4205e063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.7990566672923156    Recall: 0.913633119200571\n",
      "TSS: 0.7776166601261794    Recall: 0.922752808988764\n",
      "TSS: 0.8272890514296063    Recall: 0.9725321888412017\n",
      "TSS: 0.8241373928784765    Recall: 0.9626262626262626\n"
     ]
    }
   ],
   "source": [
    "save_results(rnn_FS_CCBR_OUS_normalized, \"rnn_FS_CCBR_OUS_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66675ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f69385b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9981, 60, 6): SVM Classifier is Done! \n",
      "\n",
      "(9050, 60, 6): SVM Classifier is Done! \n",
      "\n",
      "(10375, 60, 6): SVM Classifier is Done! \n",
      "\n",
      "(9268, 60, 6): SVM Classifier is Done! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_FS_CCBR_OUS_normalized = kfold_training('SVM', data, labels, test_data, test_labels, svm_model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cc96fbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS: 0.798096131099491    Recall: 0.939329050678087\n",
      "TSS: 0.7179384767287273    Recall: 0.8230337078651685\n",
      "TSS: 0.7828193105094566    Recall: 0.9210300429184549\n",
      "TSS: 0.8301845675402242    Recall: 0.9494949494949495\n"
     ]
    }
   ],
   "source": [
    "save_results(svm_FS_CCBR_OUS_normalized, \"svm_FS_CCBR_OUS_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78c3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
